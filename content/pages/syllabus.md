---
content_type: page
description: This syllabus section provides information on course meeting times, prerequisites,
  topics, textbooks, assignments, and grading.
draft: false
hide_download: true
hide_download_original: null
learning_resource_types: []
ocw_type: CourseSection
title: Syllabus
uid: 57047db5-41e7-df79-b250-f86dfc5554d5
---
## Course Meeting Times

Lectures: 2 sessions / week, 1.5 hours / session

## Course Information

This is a graduate-level introduction to mathematics of information theory. We will cover both classical and modern topics (such as finite blocklength IT and applications to statistical decision theory). Those taking Information Theory for the first time may benefit from reading the standard textbook by T. Cover and J. Thomas (see below).

## Course Prerequisites

This course requires knowledge of theorem-proof exposition and probability theory, as taught in [6.042J Mathematics for Computer Science](https://ocw.mit.edu/courses/6-042j-mathematics-for-computer-science-fall-2010/)Â or [6.436J Fundamentals of Probability](https://ocw.mit.edu/courses/6-436j-fundamentals-of-probability-fall-2018/).

## Course Topics

This course will cover the following topics:

1. **Information measures (entropy, divergence, mutual information)**: Convexity, monotonicity and continuity properties. Extremization, saddle point, capacity as information radius. Variational characterizations: Donsker-Varadhan and Gelfand-Yaglom- Perez. Entropy rates and theorem of Szego.
2. **Lossless data compression**: Variable length and fixed length (almost lossless). Linear compression. Slepian-Wolf problem. Ergodic sources: Shannon-McMillan and Birkhoff-Khintchine theorems. Basics of universal data compression. Optimality of Lempel-Ziv.
3. **Binary hypothesis testing:** Bounds for finite number of samples. Asymptotics: Stein and Chernoff exponents. Large deviations: Sanov, I-projection, tilting.
4. **Channel coding**: Achievability and converse bounds. Asymptotics: Capacity, strong converse, error-exponents, channel dispersion. Gaussian channels (parallel, with intersymbol interference, minimal energy-per-bit, continuous time). Coding with feedback: Zero and non-zero error capacities, Schalkwijk-Kailath and variable length codes.
5. **Lossy data compression**: Scalar quantization and Panter-Dite approximation. Vector quantization and rate-distortion theorem. Separation principle.
6. **Topics in multi-user IT**: Capacity of a multiple-access channel. Gelfand-Pinsker problem. Interference channels.

## Course Text

In addition to the lecture notes, students may find the following text to be of use. Some homework problems will be assigned from the text.

T. Cover, and J. Thomas. *Elements of Information Theory, Second Edition*. Wiley-Interscience, 2006. ISBN: 9780471241959.

## Assignments and Grading

There will be weekly assigned problem sets, one take-home midterm, and one take-home final. Collaboration is allowed on the problem sets but not on the take-home exams. The final grade will be weighted as follows:

{{< tableopen >}}{{< theadopen >}}{{< tropen >}}{{< thopen >}}
ACTIVITIES
{{< thclose >}}{{< thopen >}}
PERCENTAGES
{{< thclose >}}{{< trclose >}}{{< theadclose >}}{{< tbodyopen >}}{{< tropen >}}{{< tdopen >}}
Problem Sets
{{< tdclose >}}{{< tdopen >}}
30%
{{< tdclose >}}{{< trclose >}}{{< tropen >}}{{< tdopen >}}
Midterm Exam
{{< tdclose >}}{{< tdopen >}}
30%
{{< tdclose >}}{{< trclose >}}{{< tropen >}}{{< tdopen >}}
Final Exam
{{< tdclose >}}{{< tdopen >}}
40%
{{< tdclose >}}{{< trclose >}}{{< tbodyclose >}}{{< tableclose >}}