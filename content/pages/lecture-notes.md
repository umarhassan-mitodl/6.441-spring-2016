---
content_type: page
description: This section provides the lecture notes used for the course.
hide_download: true
hide_download_original: null
learning_resource_types:
- Lecture Notes
ocw_type: CourseSection
title: Lecture Notes
uid: 99ddf4a2-3a6e-f9c4-342b-b24697590209
---

The following lecture notes were written for 6.441 by Professors Yury Polyanskiy of MIT and Yihong Wu of University of Illinois Urbana-Champaign. A complete copy of the notes are {{% resource_link 5d8f16ad-c338-5c9f-f297-5b121bd620e4 "available for download (PDF - 7.6MB)" %}}.

{{< tableopen >}}
{{< theadopen >}}
{{< tropen >}}
{{< thopen >}}
CHAPTERS
{{< thclose >}}
{{< thopen >}}
SECTIONS
{{< thclose >}}

{{< trclose >}}

{{< theadclose >}}
{{< tropen >}}
{{< tdopen colspan="2" >}}
**Part I: Information Measures**
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
{{% resource_link 2243edff-b30f-5718-1ed9-7dcb77691580 "Chapter 1: Information measures: Entropy and divergence (PDF)" %}}
{{< tdclose >}}
{{< tdopen >}}


1.1 Entropy

1.2 Divergence

1.3 Differential entropy


{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
{{% resource_link 184197ca-5d54-18da-2415-d37e929860b9 "Chapter 2: Information measures: Mutual information (PDF)" %}}
{{< tdclose >}}
{{< tdopen >}}


2.1 Divergence: Main inequality

2.2 Conditional divergence

2.3 Mutual information

2.4 Conditional mutual information and conditional independence

2.5 Strong data-processing inequalities

2.6 How to avoid measurability problems?


{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
{{% resource_link 486b7d83-cc42-8acf-75b7-4323ecafcc11 "Chapter 3: Sufficient statistic. Continuity of divergence and mutual information. (PDF)" %}}
{{< tdclose >}}
{{< tdopen >}}


3.1 Sufficient statistics and data-processing

3.2 Geometric interpretation of mutual information

3.3 Variational characterizations of divergence: Donsker-Varadhan

3.4 Variational characterizations of divergence: Gelfand-Yaglom-Perez

3.5 Continuity of divergence. Dependence on sigma-algebra

3.6 Variational characterizations and continuity of mutual information


{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
{{% resource_link 62759bed-38f7-42cf-4ed7-2f344ba40a3e "Chapter 4: Extremization of mutual information: Capacity saddle point (PDF)" %}}
{{< tdclose >}}
{{< tdopen >}}


4.1 Convexity of information measures

4.2 Local behavior of divergence

4.3 Local behavior of divergence and Fisher information

4.4 Extremization of mutual information

4.5 Capacity = information radius

4.6 Existence of caod (general case)

4.7 Gaussian saddle point


{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
{{% resource_link b208f091-f760-52d6-0eda-895dc8d13e97 "Chapter 5: Single-letterization. Probability of error. Entropy rate. (PDF)" %}}
{{< tdclose >}}
{{< tdopen >}}


5.1 Extremization of mutual information for memoryless sources and channels

5.2 Gaussian capacity via orthogonal symmetry

5.3 Information measures and probability of error

5.4 Fano, LeCam and minimax risks

5.5 Entropy rate

5.6 Entropy and symbol (bit) error rate

5.7 Mutual information rate

5.8 Toeplitz matrices and Szego's theorem


{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen colspan="2" >}}
**Part II: Lossless Data Compression**
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
{{% resource_link 67d8e68c-d8fd-a553-66e3-f9f0a9465c17 "Chapter 6: Variable-length Lossless Compression (PDF - 1.1MB)" %}}
{{< tdclose >}}
{{< tdopen >}}


6.1 Variable-length, lossless, optimal compressor

6.2 Uniquely decodable codes, prefix codes and Huffman codes


{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
{{% resource_link 85014cb1-add9-f415-ef38-f60f678f16b2 "Chapter 7: Fixed-length (almost lossless) compression. Slepian-Wolf problem. (PDF)" %}}
{{< tdclose >}}
{{< tdopen >}}


7.1 Fixed-length code, almost lossless

7.2 Linear Compression

7.3 Compression with Side Information at both compressor and decompressor

7.4 Slepian-Wolf (Compression with Side Information at Decompressor only)

7.5 Multi-terminal Slepian Wolf

7.6 Source-coding with a helper (Ahlswede-Korner-Wyner)


{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
{{% resource_link 099eee31-611a-15cb-b951-bd427a89428f "Chapter 8: Compressing stationary ergodic sources (PDF)" %}}
{{< tdclose >}}
{{< tdopen >}}


8.1 Bits of ergodic theory

8.2 Proof of Shannon-McMillan

8.3 Proof of Birkhoff -Khintchine

8.4 Sinai's generator theorem


{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
{{% resource_link 18b243d5-e475-579b-7434-260214ad9a25 "Chapter 9: Universal compression (PDF)" %}}
{{< tdclose >}}
{{< tdopen >}}


9.1 Arithmetic coding

9.2 Combinatorial construction of Fitingof

9.3 Optimal compressors for a class of sources. Redundancy

9.4 Approximate minimax solution: Je\_reys prior

9.5 Sequential probability assignment: Krichevsky-Trofimov

9.6 Lempel-Ziv compressor


{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen colspan="2" >}}
**Part III: Binary Hypothesis Testing**
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
{{% resource_link 26fd180f-40b6-773b-f19b-659a4c5e8656 "Chapter 10: Binary hypothesis testing (PDF)" %}}
{{< tdclose >}}
{{< tdopen >}}


10.1 Binary Hypothesis Testing

10.2 Neyman-Pearson formulation

10.3 Likelihood ratio tests

10.4 Converse bounds on R(P, Q)

10.5 Achievability bounds on R(P,Q)

10.6 Asymptotics


{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
{{% resource_link d14633ba-f528-941d-cb5e-e00a334e1d89 "Chapter 11: Hypothesis testing asymptotics I (PDF)" %}}
{{< tdclose >}}
{{< tdopen >}}


11.1 Stein's regime

11.2 Chernoff regime

11.3 Basics of Large deviation theory


{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
{{% resource_link 70ca499b-4657-031e-e80d-edff6f4a5a22 "Chapter 12: Information projection and Large deviation (PDF)" %}}
{{< tdclose >}}
{{< tdopen >}}


12.1 Large-deviation exponents

12.2 Information Projection

12.3 Interpretation of Information Projection

12.4 Generalization: Sanov's theorem


{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
{{% resource_link 23be1265-744c-2e9e-8962-e79985cdda45 "Chapter 13: Hypothesis testing asymptotics II (PDF - 2.0MB)" %}}
{{< tdclose >}}
{{< tdopen >}}


13.1 (E0,E1)-Tradeoff

13.2 Equivalent forms of Theorem 13.1

13.3 Sequential Hypothesis Testing


{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen colspan="2" >}}
**Part IV: Channel Coding**
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
{{% resource_link 8d9b42a8-d43e-28ca-4a75-e09fb4a89371 "Chapter 14: Channel coding (PDF)" %}}
{{< tdclose >}}
{{< tdopen >}}


14.1 Channel Coding

14.2 Basic Results

14.3 General (Weak) Converse Bounds

14.4 General achievability bounds: Preview


{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
{{% resource_link 83c18c0b-0ff3-d650-e873-4b7bc678a2fd "Chapter 15: Channel coding: Achievability bounds (PDF)" %}}
{{< tdclose >}}
{{< tdopen >}}


15.1 Information density

15.2 Shannon's achievability bound

15.3 Dependence-testing bound

15.4 Feinstein's Lemma


{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
{{% resource_link f417578e-714d-472c-b08d-463825e45f03 "Chapter 16: Linear codes. Channel capacity. (PDF)" %}}
{{< tdclose >}}
{{< tdopen >}}


16.1 Linear coding

16.2 Channels and channel capacity

16.3 Bounds on C\_e; Capacity of Stationary Memoryless Channels

16.4 Examples of DMC

16.5 Information Stability


{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
{{% resource_link 24fb44b6-a101-e847-c32f-fe38bbeeea6e "Chapter 17: Channels with input constraints. Gaussian channels. (PDF)" %}}
{{< tdclose >}}
{{< tdopen >}}


17.1 Channel coding with input constraints

17.2 Capacity under input constraint C(P) ?= Ci(P)

17.3 Applications

17.4 Non-stationary AWGN

17.5 Stationary Additive Colored Gaussian noise channel

17.6 Additive White Gaussian Noise channel with Intersymbol Interference

17.7 Gaussian channels with amplitude constraints

17.8 Gaussian channels with fading


{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
{{% resource_link 18e413df-88c6-3035-54cc-45a6b3b876b6 "Chapter 18: Lattice codes (by O. Ordentlich) (PDF)" %}}
{{< tdclose >}}
{{< tdopen >}}


18.1 Lattice Definitions

18.2 First Attempt at AWGN Capacity

18.3 Nested Lattice Codes/Voronoi Constellations

18.4 Dirty Paper Coding

18.5 Construction of Good Nested Lattice Pairs


{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
{{% resource_link bfe9e45c-2fc4-390c-c3cc-1819c62f1686 "Chapter 19: Channel coding: Energy-per-bit, continuous-time channels (PDF - 1.1MB)" %}}
{{< tdclose >}}
{{< tdopen >}}


19.1 Energy per bit

19.2 What is N0?

19.3 Capacity of the continuous-time band-limited AWGN channel

19.4 Capacity of the continuous-time band-unlimited AWGN channel

19.5 Capacity per unit cost


{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
{{% resource_link dfddd434-5fee-8dd4-9420-be583be2b952 "Chapter 20: Advanced channel coding. Source-Channel separation. (PDF)" %}}
{{< tdclose >}}
{{< tdopen >}}


20.1 Strong Converse

20.2 Stationary memoryless channel without strong converse

20.3 Channel Dispersion

20.4 Normalized Rate

20.5 Joint Source Channel Coding


{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
{{% resource_link a5ff16d9-29ef-de1d-8313-e12e372aa94b "Chapter 21: Channel coding with feedback (PDF - 1.2MB)" %}}
{{< tdclose >}}
{{< tdopen >}}


21.1 Feedback does not increase capacity for stationary memoryless channels

21.2 Alternative proof of Theorem 21.1 and Massey's directed information

21.3 When is feedback really useful?


{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
{{% resource_link 7553afe0-0a36-3016-4b8e-774456d0fdcf "Chapter 22: Capacity-achieving codes via Forney concatenation (PDF)" %}}
{{< tdclose >}}
{{< tdopen >}}


22.1 Error exponents

22.2 Achieving polynomially small error probability

22.3 Concatenated codes

22.4 Achieving exponentially small error probability


{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen colspan="2" >}}
**Part V: Lossy Data Compression** 
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
{{% resource_link 880c3987-8ba7-dc35-a5fb-b35758d42a69 "Chapter 23: Rate-distortion theory (PDF)" %}}
{{< tdclose >}}
{{< tdopen >}}


23.1 Scalar quantization

23.2 Information-theoretic vector quantization

23.3 Converting excess distortion to average


{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
{{% resource_link aaa8d18d-decd-e45f-9713-4d3f3dcee4a3 "Chapter 24: Rate distortion: Achievability bounds (PDF)" %}}
{{< tdclose >}}
{{< tdopen >}}


24.1 Recap

24.2 Shannon's rate-distortion theorem

24.3 Covering lemma


{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
{{% resource_link 5721b7df-786b-416d-adad-7c7bb3364d00 "Chapter 25: Evaluating R(D). Lossy Source-Channel separation. (PDF)" %}}
{{< tdclose >}}
{{< tdopen >}}


25.1 Evaluation of R(D)

25.2 Analog of saddle-point property in rate-distortion

25.3 Lossy joint source-channel coding

25.4 What is lacking in classical lossy compression?


{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen colspan="2" >}}
**Part VI: Advanced Topics**
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
{{% resource_link 31c5d8ec-6bbf-9f67-c663-3ac2386351ed "Chapter 26: Multiple-access channel (PDF)" %}}
{{< tdclose >}}
{{< tdopen >}}


26.1 Problem motivation and main results

26.2 MAC achievability bound

26.3 MAC capacity region proof


{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
{{% resource_link 86f91ca4-afba-0229-a349-7e6516e28e10 "Chapter 27: Examples of MACs. Maximal Pe and zero-error capacity. (PDF)" %}}
{{< tdclose >}}
{{< tdopen >}}


27.1 Recap

27.2 Orthogonal MAC

27.3 BSC MAC

27.4 Adder MAC

27.5 Multiplier MAC

27.6 Contraction MAC

27.7 Gaussian MAC

27.8 MAC Peculiarities


{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
{{% resource_link 6b2a2c6a-7a80-dc39-f60b-e8f847f3bbd2 "Chapter 28: Random number generators (PDF)" %}}
{{< tdclose >}}
{{< tdopen >}}


28.1 Setup

28.2 Converse

28.3 Elias' construction of RNG from lossless compressors

28.4 Peres' iterated von Neumann's scheme

28.5 Bernoulli factory

28.6 Related problems


{{< tdclose >}}

{{< trclose >}}

{{< tableclose >}}